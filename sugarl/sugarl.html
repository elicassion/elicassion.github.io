<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,400,900">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans:300,400,700">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="../latex.css">
    <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/js/all.min.js"></script>
    <title>SUGARL - Active Reinforcement Learning with Limited Visual Observability</title>
</head>
<body>
    <h1>Active Reinforcement Learning with Limited Visual Observability</h1>
    
    <p class="author">
        <a href="https://www3.cs.stonybrook.edu/~jishang" target="_blank">Jinghuan Shang</a> and <a href="http://michaelryoo.com/" target="_blank">Michael Ryoo</a></p>
    <p class="author"><span class="author">Stony Brook University</span> 

   
    <div class="link-container">
        <span class="link-block">
            <a href="https://arxiv.org/abs/2306.00975" class="external-link button is-normal is-rounded is-dark" target="_blank">
                <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                </span>
                <span>Paper</span>
            </a>
        </span>

        <span class="link-block">
            <a href="https://github.com/elicassion/sugarl" class="external-link button is-normal is-rounded is-dark" target="_blank">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
            </a>
        </span>

        <span class="link-block">
            <a href="https://github.com/elicassion/active-gym" class="external-link button is-normal is-rounded is-dark" target="_blank">
                <span class="icon">
                    <i class="fa-solid fa-globe"></i>
                </span>
                <span>active-gym</span>
            </a>
        </span>
    </div>

    
    <h2>Learned policies on Atari</h2>
    <div class="video-container" id="main-video">
        <!-- <video autoplay playinline muted loop width="160" height="160">
            <source src="videos/assault_fov.mp4" type="video/mp4">
        </video>     -->
    </div>
    <caption><b>Left</b>: partial observation used; <b>Right</b>: full game</caption>
    
    <h2>Learned policies on Robosuite</h2>
    <div class="video-container" id="rs-video">
        <h3>Wiping the table</h3>
        <p>For wiping the table, we observe two types of active camera motion:
            <ul>
                <li>Videos 1-2: maintaining the location of the end effector in the frame (i.e. syncing with the robot motion).</li>
                <li>Videos 3-5: moving closer to the table and moving up to observe the table top.</li>
            </ul>
        </p>
        <video autoplay muted loop playsinline style="width: 18.8%" src="rs_videos/Wipe_sv_sync.mp4"></video>
        <video autoplay muted loop playsinline style="width: 18.8%" src="rs_videos/Wipe_sv_sync2.mp4"></video>
        <video autoplay muted loop playsinline style="width: 18.8%" src="rs_videos/Wipe_sv_move_forward_and_up.mp4"></video>
        <video autoplay muted loop playsinline style="width: 18.8%" src="rs_videos/Wipe_sv_move_forward2.mp4"></video>
        <video autoplay muted loop playsinline style="width: 18.8%" src="rs_videos/Wipe_sv_move_forward.mp4"></video>

        <p><caption class="mid"><b>Top</b>: used observation from the active camera view; <b>Bottom</b>: the same recording but from another static view for better visualization</caption></p>

        <h3>Door opening</h3>
        <p>We also observe roughly two kinds of sensory policies can be obtained for completing the task: 
            <ul>
                <li>Videos 1-3: focusing on the end-effector and the door handle.</li>
                <li>Videos 4-5: focusing on the whole robot arm and the door.</li>
            </ul>
        </p>
        <video autoplay muted loop playsinline style="width: 18.8%" src="rs_videos/Door_sv_success_check.mp4"></video>
        <video autoplay muted loop playsinline style="width: 18.8%" src="rs_videos/Door_sv_success_check2.mp4"></video>
        <video autoplay muted loop playsinline style="width: 18.8%" src="rs_videos/Door_sv_success_follow.mp4"></video>
        <video autoplay muted loop playsinline style="width: 18.8%" src="rs_videos/Door_sv_success_far.mp4"></video>
        <video autoplay muted loop playsinline style="width: 18.8%" src="rs_videos/Door_sv_success_far2.mp4"></video>

        <p><caption class="mid"><b>Top</b>: used observation from the active camera view; <b>Bottom</b>: the same recording but from another static view for better visualization</caption></p>
        

        <h3>Failure cases</h3>
        <p>We observe that the sensory policy can seek for robot end-effector although some tasks are failed.</p>
        <video style="width: 28.2%"></video>
        <video autoplay muted loop playsinline style="width: 18.8%" src="rs_videos/Door_sv_fail_find_ee.mp4"></video>
        <video autoplay muted loop playsinline style="width: 18.8%" src="rs_videos/Door_sv_fail_follow_updown.mp4"></video>
        <video style="width: 28.2%"></video>
        <p><caption class="mid"><b>Top</b>: used observation from the active camera view; <b>Bottom</b>: the same recording but from another static view for better visualization</caption></p>

    </div>
    
    
    

    <script src="js/load_videos.js"></script>

</body>
</html>